{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9795a30a-9946-4c39-bede-ec456a42d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae485d-96fc-4bdd-861b-bf9688ff6d14",
   "metadata": {},
   "source": [
    "set java home and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae81a3d2-72b7-4ed5-9558-df65c8b53937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set java home\n",
    "os.environ[\"JAVA_HOME\"]=\"C:\\Program Files\\Java\\jdk-19\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23836595-f4cf-4802-879e-43e8380cb664",
   "metadata": {},
   "source": [
    "set Spark configs detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0685d7-5bff-49d3-9546-5946bf3ec95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "        .setAppName(\"ETLPipeline\")\\\n",
    "        .setMaster(\"local\")\\\n",
    "        .set(\"spark.driver.extraClassPath\",\"C:/pyspark/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553471b-558b-4f33-86fe-28424f2684f7",
   "metadata": {},
   "source": [
    "initial spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92899823-5b74-47af-9065-39a336f11f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc  =SparkContext.getOrCreate(conf=conf)\n",
    "etl =SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0f036c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://eng-pc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETLPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x18461cf4be0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da85db2",
   "metadata": {},
   "source": [
    "# set databases detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04601c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get password from environmnet var\n",
    "pwd = os.environ['PGPASS']\n",
    "uid = os.environ['PGUID']\n",
    "#sql db details\n",
    "server = \"localhost\"\n",
    "src_db = \"AdventureWorksDW2019\"\n",
    "target_db = \"AdventureWorks\"\n",
    "src_driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "target_driver = \"org.postgresql.Driver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9253c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source connection\n",
    "src_url = f\"jdbc:sqlserver://{server}:1433;databaseName={src_db};user={uid};password={pwd};trustServerCertificate=true;\"\n",
    "#target connection\n",
    "target_url =f\"jdbc:postgresql://{server}:5432/{target_db}?user=etl&password=demopass?client\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80ea02",
   "metadata": {},
   "source": [
    "# sql statement and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583f0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"select  t.name as table_name from sys.tables t \n",
    "where t.name in ('DimProduct','DimProductSubcategory','DimProductCategory','DimSalesTerritory','FactInternetSales') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ee57ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          table_name|\n",
      "+--------------------+\n",
      "|          DimProduct|\n",
      "|  DimProductCategory|\n",
      "|DimProductSubcate...|\n",
      "|   DimSalesTerritory|\n",
      "|   FactInternetSales|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test our connection\n",
    "dfs=etl.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    options(driver=src_driver, user=uid, password=pwd, url=src_url, query=sql). \\\n",
    "    load()\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34ab8d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimProduct\n",
      "DimProductCategory\n",
      "DimProductSubcategory\n",
      "DimSalesTerritory\n",
      "FactInternetSales\n"
     ]
    }
   ],
   "source": [
    "data_collect = dfs.collect()\n",
    "# looping thorough each row of the dataframe\n",
    "for row in data_collect:\n",
    "    # while looping through each\n",
    "    # row printing the data of table_name\n",
    "    print(row[\"table_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf16fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    try:\n",
    "        dfs=etl.read. \\\n",
    "            format(\"jdbc\"). \\\n",
    "            options(driver=src_driver,user=uid, password=pwd,url=src_url,query=sql). \\\n",
    "            load()\n",
    "        # get table names\n",
    "        data_collect = dfs.collect()\n",
    "        # looping thorough each row of the dataframe\n",
    "        for row in data_collect:\n",
    "        # while looping through each\n",
    "        # row printing the data of table_name\n",
    "            print(row[\"table_name\"])\n",
    "            tbl_name = row[\"table_name\"]\n",
    "            df = etl.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"driver\", src_driver) \\\n",
    "            .option(\"user\", uid) \\\n",
    "            .option(\"password\", pwd) \\\n",
    "            .option(\"url\", src_url) \\\n",
    "            .option(\"dbtable\", f\"dbo.{tbl_name}\") \\\n",
    "            .load()\n",
    "            #print(df.show(10))\n",
    "            load(df, tbl_name)\n",
    "            print(\"Data loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(\"Data extract error: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe4f327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(df, tbl):\n",
    "    try:\n",
    "        rows_imported = 0\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + df.count()}... for table {tbl}')\n",
    "        df.write.mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", target_url) \\\n",
    "        .option(\"user\", uid) \\\n",
    "        .option(\"password\", pwd) \\\n",
    "        .option(\"driver\", target_driver) \\\n",
    "        .option(\"dbtable\", \"src_\" + tbl) \\\n",
    "        .save()\n",
    "        print(\"Data imported successful\")\n",
    "        rows_imported += df.count()\n",
    "    except Exception as e:\n",
    "        print(\"Data load error: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88c9e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimProduct\n",
      "importing rows 0 to 606... for table DimProduct\n",
      "Data imported successful\n",
      "Data loaded successfully\n",
      "DimProductCategory\n",
      "importing rows 0 to 4... for table DimProductCategory\n",
      "Data imported successful\n",
      "Data loaded successfully\n",
      "DimProductSubcategory\n",
      "importing rows 0 to 37... for table DimProductSubcategory\n",
      "Data imported successful\n",
      "Data loaded successfully\n",
      "DimSalesTerritory\n",
      "importing rows 0 to 11... for table DimSalesTerritory\n",
      "Data imported successful\n",
      "Data loaded successfully\n",
      "FactInternetSales\n",
      "importing rows 0 to 60398... for table FactInternetSales\n",
      "Data imported successful\n",
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "extract()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
